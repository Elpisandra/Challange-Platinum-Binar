# -*- coding: utf-8 -*-
"""CNN Training

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19TK9l3be7kzrWQcRNNDnJFIIcNCwKVgR

# Import Library and Load Data
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import scipy.stats as st
import regex as re

df= pd.read_csv('/content/drive/MyDrive/PLATINUM/data (1).csv',encoding='latin1')
df_train=pd.read_csv('/content/drive/MyDrive/PLATINUM/train_preprocess.tsv.txt',sep='\t',header=None)
df_train=df_train.rename(columns={0:'text'})
df_train=df_train.rename(columns={1:'label'})
alay_dict = pd.read_csv('/content/drive/MyDrive/PLATINUM/new_kamusalay.csv', encoding='latin-1', header=None)
alay_dict = alay_dict.rename(columns={0: 'original',
                                      1: 'replacement'})

df_train.isnull().sum()

df_train.duplicated().sum()

df_train = df_train.drop_duplicates()
df_train.duplicated().sum()

"""# Data Cleansing

##Regex
"""

def regex(text):
  text = re.sub('USER', '', text) #Remove USER
  text = re.sub('RT', '', text) #Remove RT
  text = re.sub('URL', '', text) #Remove URL
  text = re.sub(r'\\n+', '', text) #Remove \n
  text = re.sub(r'https\S+','', text) #Remove https
  text = re.sub(r'\\x[A-Za-z0-9./]+', '', text) #Remove \x96 etc
  text = re.sub('#[A-Za-z0-9./]+', '', text) #Remove hastag
  text = re.sub('  +', '', text) #Remove extra space
  text = text.lower() #Lowercase text
  text = re.sub('[^a-zA-Z]+', ' ', text) #Remove non alpha numeric
  return text
df_train['text'] = df_train['text'].apply(regex)

"""##Normalization"""

alay_dict_map = dict(zip(alay_dict['original'], alay_dict['replacement']))
def normalize_alay(text):
    return ' '.join([alay_dict_map[word] if word in alay_dict_map else word for word in text.split(' ')])

def normalization(text):
    text = normalize_alay(text)
    return text
df_train['text'] = df_train['text'].apply(normalization)

"""## Stopword"""

import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords as stopwords_scratch
list_stopwords=stopwords_scratch.words('indonesian')
list_stopwords.extend(['ya','yg','ga','yuk','dah','sih','gue','nya','nih'])
stopwords=list_stopwords

def remove_stopword(text):
    text = ' '.join(['' if word in stopwords else word for word in text.split(' ')])
    text = re.sub('  +', ' ', text)
    text = text.strip()
    return text
def stopword(text):
    text = remove_stopword(text)
    return text
df_train['text'] = df_train['text'].apply(stopword)

"""# CNN"""

neg=df_train.loc[df_train['label']=='negative'].text.tolist()
neu=df_train.loc[df_train['label']=='neutral'].text.tolist()
pos=df_train.loc[df_train['label']=='positive'].text.tolist()
neg_label=df_train.loc[df_train['label']=='negative'].label.tolist()
neu_label=df_train.loc[df_train['label']=='neutral'].label.tolist()
pos_label=df_train.loc[df_train['label']=='positive'].label.tolist()

total_data=pos+neu+neg
labels=pos_label+neu_label+neg_label
print("Pos: %s, Neu: %s, Neg: %s" % (len(pos),len(neu),len(neg)))
print("Total data: %s" % len(total_data))

"""## Feature extraction"""

import pickle
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from collections import defaultdict

max_features=10000
tokenizer=Tokenizer(num_words=max_features,split=' ',lower=True)
tokenizer.fit_on_texts(total_data)
with open('tokenizer.pickle','wb') as handle:
  pickle.dump(tokenizer,handle,protocol=pickle.HIGHEST_PROTOCOL)
  print("Tokenizer.pickle has created!")

X=tokenizer.texts_to_sequences(total_data)
vocab_size=len(tokenizer.word_index)
maxlen=max(len(x) for x in X)

X=pad_sequences(X)
with open('x_pad_sequences.pickle','wb') as handle:
  pickle.dump(X,handle,protocol=pickle.HIGHEST_PROTOCOL)
  print("X_pad_sequnces.pickle has created!")

Y=pd.get_dummies(labels)
Y=Y.values

with open('y_labels.pickle','wb') as handle:
  pickle.dump(Y,handle,protocol=pickle.HIGHEST_PROTOCOL)
  print("Y_labels.pickle has created")

"""## Training Data"""

from sklearn.model_selection import train_test_split
file=open("x_pad_sequences.pickle",'rb')
X=pickle.load(file)
file.close()
file=open("y_labels.pickle",'rb')
Y=pickle.load(file)
file.close()
X_train,X_test,y_train,y_test=train_test_split(X,Y,test_size=0.3,random_state=1)

import numpy as np
from tensorflow.keras import layers
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense,Embedding,LSTM,SpatialDropout1D
from tensorflow.keras import optimizers
from tensorflow.keras.callbacks import EarlyStopping,TensorBoard
from tensorflow.keras.layers import Flatten
from tensorflow.keras import backend as K
from tensorflow.keras.layers import Conv1D,MaxPooling1D,GlobalMaxPooling1D
import tensorflow as tf
from tensorflow.keras import regularizers

embed_dim=64
model=Sequential()
model.add(Embedding(max_features,embed_dim,input_length=maxlen))
model.add(layers.Conv1D(128,3,activation='relu'))
model.add(layers.GlobalMaxPooling1D())
model.add(layers.Dropout(0.5))
model.add(layers.Dense(32,activation='relu'))
model.add(layers.Dense(3,activation='softmax'))
model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])
print(model.summary())
es=EarlyStopping(monitor='val_loss',mode='min',verbose=1,patience=1)
history=model.fit(X_train,y_train,epochs=10,batch_size=32,validation_data=(X_test,y_test),verbose=1,callbacks=[es])

import numpy as np
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score
from sklearn.model_selection import KFold

kf=KFold(n_splits=5,random_state=42,shuffle=True)
accuracies=[]
y=Y
embed_dim=64

for iteration,data in enumerate(kf.split(X),start=1):
  data_train=X[data[0]]
  target_train=y[data[0]]
  data_test=X[data[1]]
  target_test=y[data[1]]

  model=Sequential()
  model.add(Embedding(max_features,embed_dim,input_length=maxlen))
  model.add(layers.Conv1D(128,3,activation='relu'))
  model.add(layers.GlobalMaxPooling1D())
  model.add(layers.Dropout(0.5))
  model.add(layers.Dense(32,activation='relu'))
  model.add(layers.Dense(3,activation='softmax'))
  model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])
  print(model.summary())
  es=EarlyStopping(monitor='val_loss',mode='min',verbose=1,patience=1)
  history=model.fit(X_train,y_train,epochs=10,batch_size=32,validation_data=(X_test,y_test),verbose=1,callbacks=[es])
  predictions=model.predict(X_test)
  y_pred=predictions
  accuracy=accuracy_score(y_test.argmax(axis=1),y_pred.argmax(axis=1))

  print("Training ke-",iteration)
  print(classification_report(y_test.argmax(axis=1),y_pred.argmax(axis=1)))
  print("============================================================================================================")
  accuracies.append(accuracy)
average_accuracy=np.mean(accuracies)
print()
print()
print()
print("Rata-rata Accuracy:",average_accuracy)

"""## Evaluasi"""

from sklearn import metrics
predictions=model.predict(X_test)
y_pred=predictions
matrix_test=metrics.classification_report(y_test.argmax(axis=1),y_pred.argmax(axis=1))
print("Testing selesai")
print(matrix_test)

# Commented out IPython magic to ensure Python compatibility.
# history.history
import matplotlib.pyplot as plt
plt.style.use('ggplot')

def plot_history(history):
  acc=history.history['accuracy']
  val_acc=history.history['val_accuracy']
  loss=history.history['loss']
  val_loss=history.history['val_loss']
  x=range(1,len(acc)+1)

  plt.figure(figsize=(12,5))
  plt.subplot(1,2,1)
  plt.plot(x,acc,'b',label="Training acc")
  plt.plot(x,val_acc,'r',label="Validation acc")
  plt.title("Training and validation accuracy")
  plt.legend()
  plt.subplot(1,2,2)
  plt.plot(x,loss,'b',label="Training loss")
  plt.plot(x,val_loss,'r',label="Validation loss")
  plt.title("Training and validation loss")
  plt.legend()
# %matplotlib inline
plot_history(history)

model.save('model.h5')
print("Model has created")

"""## Predict"""

from keras.models import load_model
import re
sentiment=['negative','neutral','positive']
input_text='''syukur'''
def regex(sent):
  text = sent.lower() #Lowercase text
  text = re.sub('USER', '', text) #Remove USER
  text = re.sub('RT', '', text) #Remove RT
  text = re.sub('URL', '', text) #Remove URL
  text = re.sub(r'\\n+', '', text) #Remove \n
  text = re.sub(r'https\S+','', text) #Remove https
  text = re.sub(r'\\x[A-Za-z0-9./]+', '', text) #Remove \x96 etc
  text = re.sub('#[A-Za-z0-9./]+', '', text) #Remove hastag
  text = re.sub('  +', '', text) #Remove extra space
  text = re.sub('[^a-zA-Z]+', ' ', text) #Remove non alpha numeric
  return text
text=[regex(input_text)]
predicted=tokenizer.texts_to_sequences([text])
guess=pad_sequences(predicted,maxlen=X.shape[1])
model=load_model('model.h5')
prediction=model.predict(guess)
polarity=np.argmax(prediction)
print("Text: ",text[0])
print("Sentiment: ",sentiment[polarity])

"""# Visualisasi"""

from wordcloud import WordCloud
text = ' '.join(df_train['text'])
wordcloud = WordCloud().generate(text)
plt.imshow(wordcloud)
plt.axis("off")
plt.show

df_train_positive = df_train[df_train['label'] == 'positive']
df_train_positive = df_train_positive[['text', 'label']]
df_train_neutral = df_train[df_train['label'] == 'neutral']
df_train_neutral = df_train_neutral[['text', 'label']]
df_train_negative = df_train[df_train['label'] == 'negative']
df_train_negative = df_train_negative[['text', 'label']]

from wordcloud import WordCloud
text = ' '.join(df_train_positive['text'])
wordcloud = WordCloud().generate(text)
plt.imshow(wordcloud)
plt.axis("off")
plt.show

from wordcloud import WordCloud
text = ' '.join(df_train_neutral['text'])
wordcloud = WordCloud().generate(text)
plt.imshow(wordcloud)
plt.axis("off")
plt.show

from wordcloud import WordCloud
text = ' '.join(df_train_negative['text'])
wordcloud = WordCloud().generate(text)
plt.imshow(wordcloud)
plt.axis("off")
plt.show

df['Tweet'] = df['Tweet'].apply(regex)
df['Tweet'] = df['Tweet'].apply(normalization)
df['Tweet'] = df['Tweet'].apply(remove_stopword)

df_hs_individual = df[df['HS_Individual'] == 1]
df_hs_individual = df_hs_individual[['Tweet', 'HS_Individual']]
df_hs_religion = df[df['HS_Religion'] == 1]
df_hs_religion = df_hs_religion[['Tweet', 'HS_Religion']]
df_hs_weak = df[df['HS_Weak'] == 1]
df_hs_weak = df_hs_weak[['Tweet', 'HS_Weak']]

from wordcloud import WordCloud
text = ' '.join(df_hs_individual['Tweet'])
wordcloud = WordCloud().generate(text)
plt.imshow(wordcloud)
plt.axis("off")
plt.show

from wordcloud import WordCloud
text = ' '.join(df_hs_religion['Tweet'])
wordcloud = WordCloud().generate(text)
plt.imshow(wordcloud)
plt.axis("off")
plt.show

from wordcloud import WordCloud
text = ' '.join(df_hs_weak['Tweet'])
wordcloud = WordCloud().generate(text)
plt.imshow(wordcloud)
plt.axis("off")
plt.show