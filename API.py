# -*- coding: utf-8 -*-
"""API

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AdNCICrBOC0-IaDn2_fOyxCzOPpTXVCx
"""

# Import Regex, Sqlite, Pandas, Numpy, & Tensorflow
import pickle, re
import sqlite3
import pandas as pd
import numpy as np
from tensorflow.keras.preprocessing.text import Tokenizer
from keras.models import load_model
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Import library for Flask
from flask import Flask, jsonify
from flask import request
from flasgger import Swagger, LazyString, LazyJSONEncoder
from flasgger import swag_from

# Define Swagger UI description
app = Flask(__name__)
app.json_encoder = LazyJSONEncoder
swagger_template = dict(
info = {
    'title': LazyString(lambda: 'API Documentation for Sentiment Analysis'),
    'version': LazyString(lambda: '1.0.0'),
    'description': LazyString(lambda: 'API Documentation for Sentiment Analysis'),
    },
    host = LazyString(lambda: request.host)
)
swagger_config = {
    "headers": [],
    "specs": [
        {
            "endpoint": 'docs',
            "route": '/docs.json',
        }
    ],
    "static_url_path": "/flasgger_static",
    "swagger_ui": True,
    "specs_route": "/docs/"
}
swagger = Swagger(app, template=swagger_template,
                  config=swagger_config)

# Connect to db
conn = sqlite3.connect('data/contoh.db', check_same_thread=False)
conn.row_factory = sqlite3.Row
mycursor = conn.cursor()

# Define and execute query for create table "data" if not exist
# Table "Data" contains "text" coloumn and "text_clean" coloumn. The two columns have "varchar" data type
conn.execute('''CREATE TABLE IF NOT EXISTS data (text varchar(255), text_clean varchar(255),sentiment varchar(255));''')

#Tokenizing
max_features=100000
tokenizer=Tokenizer(num_words=max_features,split=' ',lower=True)

sentiment=['negative','neutral','positive']

#Feature Extraction & Load Model CNN
file=open("resource_of_cnn/x_pad_sequences.pickle",'rb')
feature_file_from_cnn=pickle.load(file)
file.close()

model_file_from_cnn=load_model("model_of_cnn/model.h5")

#Feature Extraction & Load Model LSTM
file=open("resource_of_lstm/x_pad_sequences.pickle",'rb')
feature_file_from_lstm=pickle.load(file)
file.close()

model_file_from_lstm=load_model("model_of_lstm/model.h5")

#Stopword
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords as stopwords_scratch
list_stopwords=stopwords_scratch.words('indonesian')
list_stopwords.extend(['ya','yg','ga','yuk','dah','sih','gue','nya','nih'])
stopwords=list_stopwords

def remove_stopword(text):
    text = ' '.join(['' if word in stopwords else word for word in text.split(' ')])
    text = re.sub('  +', ' ', text)
    text = text.strip()
    return text

#Regex
def data_cleaning(text):
  text = re.sub('USER', '', text)
  text = re.sub('RT', '', text)
  text = re.sub('URL', '', text)
  text = re.sub(r'\\n+','', text)
  text = re.sub(r'https\S+','', text)
  text = re.sub(r'\\x[A-Za-z0-9./]+', '', text)
  text = re.sub('#[A-Za-z0-9./]+', '', text)
  text = re.sub('  +', '', text)
  return text.lower()

def data_cleaning2(text):
  text = re.sub('[^a-zA-Z]+', ' ', text)
  return text

def preprocess(text):
    text = data_cleaning(text)
    text = data_cleaning2(text)
    text = remove_stopword(text)
    return text

def cnn_file(input_file):
  column = input_file.iloc[:,0]
  print(column)

  for data in column:
    text_clean = preprocess(data)
    feature=tokenizer.texts_to_sequences(text_clean)
    feature_pad_sequences=pad_sequences(feature,maxlen=feature_file_from_cnn.shape[1])
    prediction=model_file_from_cnn.predict(feature_pad_sequences)
    get_sentiment=sentiment[np.argmax(prediction[0])]
    query_text = "insert into data(text, text_clean,sentiment) values(?,?,?)"
    val = (data, text_clean, get_sentiment)
    mycursor.execute(query_text, val)
    conn.commit()
    print(data)

def lstm_file(input_file):
  column = input_file.iloc[:,0]
  print(column)

  for data in column:
    text_clean = preprocess(data)
    feature=tokenizer.texts_to_sequences(text_clean)
    feature_pad_sequences=pad_sequences(feature,maxlen=feature_file_from_lstm.shape[1])
    prediction=model_file_from_lstm.predict(feature_pad_sequences)
    get_sentiment=sentiment[np.argmax(prediction[0])]
    query_text = "insert into data(text, text_clean,sentiment) values(?,?,?)"
    val = (data, text_clean, get_sentiment)
    mycursor.execute(query_text, val)
    conn.commit()
    print(data)

# Define endpoint for "input teks via form"
@swag_from("docs/cnn_text.yml", methods=['POST'])
@app.route('/cnn_text', methods=['POST'])
def text_processing_cnn():
  # Get text file
  text = request.form.get('text')

  # Cleansing Process. Remove "Emoticon" and "Punctuation"
  text_clean = preprocess(text)

  #Tokenizing and Pad Sequences
  feature=tokenizer.texts_to_sequences(text_clean)
  feature=pad_sequences(feature,maxlen=feature_file_from_cnn.shape[1])

  #Predict Sentiment
  prediction=model_file_from_cnn.predict(feature)
  get_sentiment=sentiment[np.argmax(prediction[0])]

  # Define and execute query for insert original text and cleaned text to sqlite database
  conn.execute("INSERT INTO data (text, text_clean) VALUES ('" + text + "', '" + text_clean + "')")
  conn.commit()

  # Define API response
  json_response = {
      'status_code': 200,
      'description': "Result of Sentiment Analysis using CNN",
      'data': {
          'text':text,
          'sentiment':get_sentiment
      },
  }
  response_data = jsonify(json_response)
  return response_data

@swag_from("docs/cnn_file.yml", methods=['POST'])
@app.route('/cnn_file', methods=['POST'])
def input_csv_cnn():
  file = request.files['file']

  try:
    data_csv = pd.read_csv(file, encoding='iso-8859-1')
  except:
    data_csv = pd.read_csv(file, encoding='utf-8')
  cnn_file(data_csv)
  query_text = "select * from data"
  select_data = mycursor.execute(query_text)
  conn.commit
  data=[
      dict(text_clean=row[1],sentiment=row[2])
  for row in select_data.fetchall()
  ]

  return jsonify(data)

@swag_from("docs/lstm_text.yml", methods=['POST'])
@app.route('/lstm_text', methods=['POST'])
def text_processing_lstm():

    # Get text file
    text = request.form.get('text')

    # Cleansing Process. Remove "Emoticon" and "Punctuation"
    text_clean = preprocess(text)

    feature=tokenizer.texts_to_sequences(text_clean)
    feature=pad_sequences(feature,maxlen=feature_file_from_lstm.shape[1])

    prediction=model_file_from_lstm.predict(feature)
    get_sentiment=sentiment[np.argmax(prediction[0])]

    # Define and execute query for insert original text and cleaned text to sqlite database
    conn.execute("INSERT INTO data (text, text_clean) VALUES ('" + text + "', '" + text_clean + "')")
    conn.commit()

    # Define API response
    json_response = {
        'status_code': 200,
        'description': "Result of Sentiment Analysis using LSTM",
        'data': {
            'text':text,
            'sentiment':get_sentiment
        },
    }
    response_data = jsonify(json_response)
    return response_data

@swag_from("docs/lstm_file.yml", methods=['POST'])
@app.route('/lstm_file', methods=['POST'])
def input_csv_lstm():
  file = request.files['file']

  try:
    data_csv = pd.read_csv(file, encoding='iso-8859-1')
  except:
    data_csv = pd.read_csv(file, encoding='utf-8')
  lstm_file(data_csv)
  query_text = "select * from data"
  select_data = mycursor.execute(query_text)
  conn.commit
  data=[
      dict(text_clean=row[1],sentiment=row[2])
  for row in select_data.fetchall()
  ]

  return jsonify(data)

if __name__ == '__main__':
   app.run()